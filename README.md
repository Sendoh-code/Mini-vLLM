# ğŸš€ mini-vLLM â€” A Minimal yet Functional vLLM-Style Inference Engine

**mini-vLLM** is a lightweight educational re-implementation of the core ideas behind the vLLM inference engine.
It focuses on **understanding**, not performance, and faithfully reproduces the essential mechanisms of modern high-performance LLM inference:

* **Prefill + incremental decode** execution
* **KV-cache construction, slicing, and gathering**
* **Batch inference over multiple requests**
* **ChatML-formatted prompts for Instruct models**
* **Global token/KV/request state management**
* **A scheduler-like control flow (request lifecycle management)**

The codebase avoids CUDA kernels, PagedAttention, memory paging, and other optimizations so that the core logic remains transparent and easy to learn.

---

# âœ¨ Features

### âœ” Prefill (full-sequence forward pass)

Runs a full forward pass to initialize:

* per-request KV caches
* the next token for each request

One prefill per request, just like vLLM.

---

### âœ” Incremental decoding (one token at a time)

Each decode step receives:

* the last token for each active request
* the gathered KV cache for the batch

and returns:

* one new token per request
* updated KV cache tensors (32 layers for Mistral-7B)

---

### âœ” KV-cache management (slice + gather)

Fully supports the Transformers 4.36 **tuple-based KV format**:

* `slice_kv()` extracts the KV tensors for a single request
* `gather_kv()` stacks multiple KV caches into a batch layout

This mirrors vLLMâ€™s logical KV-management behavior (without paging).

---

### âœ” ChatML prompt formatting (Instruct-model-friendly)

All user prompts are wrapped as:

```
<s>[INST] user_message [/INST]
```

This is necessary to make Mistral Instruct behave like a chat assistant instead of generating Q/A lists or essays.

---

### âœ” Global state tracking

Three centralized state stores:

* **TokenManager:** `req_id â†’ token_id list`
* **KVManager:** `req_id â†’ tuple-of-KV tensors`
* **RequestTable:** metadata (prompt, finished flag, etc.)

This design matches real LLM serving architectures.

---

### âœ” Scheduler-style control flow

The engine implements a simplified scheduler that:

* tracks active vs. finished requests
* checks EOS tokens
* updates token/KV states each iteration
* reconstructs batch KV with `gather_kv()`

This architecture is intentionally compatible with full vLLM-style dynamic batching.

---

# ğŸ“ Project Structure

```
mini-vllm/
â”‚
â”œâ”€â”€ model_runner.py      # Pure inference: prefill() + decode_step()
â”œâ”€â”€ utils.py             # KV slicing/gathering utilities
â””â”€â”€ engine.py            # MiniVLLMEngine (state + scheduling logic)
```

---

# ğŸ§  Inference Workflow

### 1. Register a request

Applies ChatML formatting and tokenizes the prompt.

```python
req_id = engine.register_request("Whatâ€™s your name?")
```

---

### 2. Prefill

Builds the initial KV cache.

```python
engine.prefill([req_id])
```

---

### 3. Decode loop

Decodes step-by-step, updating per-request KV and token streams.

```python
for _ in range(30):
    engine.decode_step([req_id])
```

---

### 4. Retrieve final text

```python
print(engine.get_text(req_id))
```

Example output:

```
<s><s>[INST] What's your name? [/INST] 
My name is Mistral 7B v0.1. But you can call me Mistral.</s>
```

---

# ğŸ›  Requirements

* Python 3.10+
* PyTorch 2.x
* transformers == 4.36.x
* CUDA GPU (recommended but not required)

---

# ğŸ“Œ Notes

* This project focuses on **logic clarity**, not speed.
* It intentionally omits:

  * PagedAttention
  * GPU memory paging
  * Multi-GPU execution
  * CUDA kernel fusion

The goal is to make the inference architecture fully understandable.

---

# ğŸ§­ Roadmap

* [ ] Complete Scheduler (dynamic batching + queueing)
* [ ] EOS / stop-token handling
* [ ] Output post-processing (remove ChatML markers)
* [ ] True multi-request decode demonstration
* [ ] Prefix-sharing experiments
* [ ] KV-paging simulator
* [ ] Throughput comparison (HF generate vs. mini-vLLM)

---

# ğŸ¤ Acknowledgements

Inspired by **vLLM**, **HuggingFace Transformers**, and recent literature on efficient LLM inference.
The implementation mirrors real production systems while remaining compact and easy to study.

---

# ğŸ“œ License

MIT License.

---

If you'd like, I can also prepare:

### âœ” A shorter README

### âœ” A more academic IEEE/NeurIPS-style README

### âœ” A README with diagrams

### âœ” A README including installation commands and examples

### âœ” A Chinese version

Just tell me what style you prefer.
